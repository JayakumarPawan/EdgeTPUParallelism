{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import pickle\n",
    "import architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2460 - accuracy: 0.9280 - val_loss: 0.1196 - val_accuracy: 0.9635\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.1036 - accuracy: 0.9679 - val_loss: 0.0932 - val_accuracy: 0.9705\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0726 - accuracy: 0.9769 - val_loss: 0.0902 - val_accuracy: 0.9716\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0546 - accuracy: 0.9826 - val_loss: 0.0917 - val_accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 7s 4ms/step - loss: 0.0413 - accuracy: 0.9863 - val_loss: 0.0893 - val_accuracy: 0.9736\n",
      "saved model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.engine.sequential.Sequential at 0x7effb0422790>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#one time training of the model can skip if model is already trained\n",
    "def create_model():\n",
    "    # Load MNIST dataset\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    # Normalize the input data\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    model = architectures.simple()\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(\"models/mnist_dnn.keras\")\n",
    "    print('saved model')\n",
    "    return model\n",
    "\n",
    "create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps2p2wjab/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmps2p2wjab/assets\n",
      "/u/pj8wfq/rl/MC/lib/python3.9/site-packages/tensorflow/lite/python/convert.py:953: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "2024-04-22 18:23:31.003579: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2024-04-22 18:23:31.003612: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2024-04-22 18:23:31.003917: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmps2p2wjab\n",
      "2024-04-22 18:23:31.005032: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2024-04-22 18:23:31.005051: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmps2p2wjab\n",
      "2024-04-22 18:23:31.007164: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2024-04-22 18:23:31.024989: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmps2p2wjab\n",
      "2024-04-22 18:23:31.031465: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 27549 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 2, Total Ops 9, % non-converted = 22.22 %\n",
      " * 2 ARITH ops\n",
      "\n",
      "- arith.constant:    2 occurrences  (f32: 2)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 2)\n",
      "\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved tf lite model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    }
   ],
   "source": [
    "#load the pretrained model and split it into two\n",
    "def split_model():\n",
    "    # Load the saved model\n",
    "    model = tf.keras.models.load_model(\"models/mnist_dnn.keras\")\n",
    "    \n",
    "    # Split the model into two\n",
    "    model1 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "    ])\n",
    "    # print(model1.predict(tf.random.normal([1, 28, 28])))\n",
    "    \n",
    "    model2 = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(64,input_shape=(128,), activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    # print(model.summary())\n",
    "    # print(model1.summary())\n",
    "    # print(model2.summary())\n",
    "    return model1, model2\n",
    "\n",
    "#convert one model to tflite\n",
    "def convert_to_tflite(model):\n",
    "    def representative_dataset():\n",
    "        for _ in range(100):\n",
    "            yield [tf.random.normal([1, 28, 28]),]\n",
    "\n",
    "    quantizer = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    quantizer.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    quantizer.representative_dataset = representative_dataset\n",
    "    quantizer.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "    quantizer.inference_input_type = tf.uint8  # or tf.uint8\n",
    "    quantizer.inference_output_type = tf.uint8  # or tf.uint8\n",
    "    \n",
    "    tflite_quant_model = quantizer.convert()\n",
    "\n",
    "    # Save the TFLite model to a file\n",
    "    with open(\"mnist_model_batched.tflite\", \"wb\") as f:\n",
    "        f.write(tflite_quant_model)\n",
    "    print(\"saved tf lite model\")\n",
    "    return tflite_quant_model\n",
    "\n",
    "\n",
    "model_keras, model_tflite = split_model()\n",
    "model_keras.save(\"models/mnist_dnn1a.keras\")\n",
    "model_tflite = convert_to_tflite(model_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 128)\n",
      "average inference time:  0.06477290317416191  ms\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "def generate_input(n):\n",
    "    return np.random.rand(n, 28, 28).astype(np.float32)\n",
    "#load model 1\n",
    "\n",
    "model_1a = tf.keras.models.load_model(\"models/mnist_dnn1a.keras\")\n",
    "start = time.perf_counter()\n",
    "outputs = model_1a.predict(generate_input(n_samples), verbose=0)\n",
    "print(outputs.shape)\n",
    "end = time.perf_counter()\n",
    "print(\"average inference time: \", (end - start)*1000/n_samples, \" ms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average inference time:  0.029266998171806335 ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_1b_input():\n",
    "    return np.random.rand(1,128).astype(np.float32)\n",
    "\n",
    "def tflite_inference(num_trials):\n",
    "    \n",
    "    #load model 1\n",
    "    model_1a = tf.keras.models.load_model(\"models/mnist_dnn1a.keras\")\n",
    "    \n",
    "    \n",
    "    # Load the TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"mnist_model_batched.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # Get input and output details\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    output_scale, output_zero_point = output_details[0]['quantization']\n",
    "    input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "    \n",
    "    # Prepare input data\n",
    "    input_shape = input_details[0]['shape']\n",
    "    inference_times = []\n",
    "    inputs = [generate_1b_input() for _ in range(num_trials)]\n",
    "    for input_data in inputs:\n",
    "        start = time.perf_counter()\n",
    "        input_data = (input_data / input_scale) + input_zero_point\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data.astype(np.uint8))\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        output_data = output_scale * (output_data - output_zero_point)\n",
    "        end = time.perf_counter()\n",
    "        inference_times.append((end - start)*1000)\n",
    "\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    print(\"average inference time: \", avg_inference_time, \"ms\")\n",
    "\n",
    "tflite_inference(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
